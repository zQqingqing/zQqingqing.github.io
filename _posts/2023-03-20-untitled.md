---
layout: post
read_time: true
show_date: true
title:  Least Squares (pseudoinverse)
date:   2023-03-19 19:30:20 -0600
description: Least Squares and the first way to solve it.
tags: [Least Squares]
author: zQqingqing
img: posts/20230320/Bigpic.png 
github:  
mathjax: yes
---
# Least Squares

This is note1 based on  *Linear Algebra And Learning From Data* by *Gilbert Strang*.

---

### Least Squares
Many applications lead to **unsolvable** linear equations $Ax=b$. The least squares method chooses $\hat x$ to make $||b-A\hat x||^2$ as **small** as possible.    Which is $(Ax-b)^T(Ax-b)$. Minimizing erorr means its derivatives are zero which leads to **normal eqautions** $A^TA\hat x=A^Tb$.   

### Four ways to solve  
1.The SVD of A leads to its **pseudoinverse** $A^+$. Then $\hat x = A^+b$ :one short formula (This note)  
2.$A^TA\hat x=A^Tb$ can be solved directly when A has **independent columns**  
3.The Gram-Schmidt idea produces **orthogonal columns** in Q which is $A=QR$.
4.**Minimize** $||b-Ax||^2 + \delta^2||x||^2$ 
#### $A^+$ is Pseudoinverse of A

Recall from four subspaces
<center><img src='./assets/img/posts/20230320/foursubspaces.png'></center>

When A multiplies a vector x in its rowspace (otherwise Ax = 0), this produces Ax in the column space. If A is invertible, then $A^+ =A^{-1}$, and we have $A^+Ax=x$ **exactly when x is in the row space**. And $AA^+b=b$ when b is in the column space.  
We conclude:  
$A$: Row space to column space  
$A^+$: Column space to row space  
<center><img src='./assets/img/posts/20230320/A.png'></center>

#### Rules to get pseudoinverse
1. If A has independent columns, then $A^+=(A^TA)^{-1}A^T$ and $A^+A=I$
2. If A has independent rows, then $A^+=A^T(AA^T)^{-1}$ and $AA^+=I$
3. A diagonal matrix $\sum_{m*n}$ has $\sum ^+ _{n*m}$  

for **all matrices**: $A=U\sum V^T$ (SVD) , $A^+=V\sum^+U^T$

##### The Least Squares Solution to $Ax=b$ is $x^+ = A^+B$  
properties:  
$x^+=A^+b$ makes $||b-Ax||^2$ as small as possible.  
If another $\hat x$ achieves then $||x^+||\le||\hat x||$ (Minimum norm)

We're going to demonstrate these properties.  

**Squared error** 
$||b-Ax||^2=||b-U\Sigma V^Tx||^2=||U^Tb-\Sigma V^Tx||^2$ (since $U^TU=I$, multiply by $U^T$ doesn't change its length)  
Set $w=V^Tx$ to get $||U^Tb-\Sigma w||^2$ , and the best $w=\Sigma ^+U^T b$ which leads to $x^+ = A^+b$  

When A has rank $r=n$, this is the only least square solution. But if there are nonezero vectors x in the nullspace of A (which implies $r < n$), they can be added to $x^+$. the error $||b-A(x+x^+)||$ is not affected because $Ax = 0$. But the norm will grow to $||x^+||^2+||x||^2$.  




